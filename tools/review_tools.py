"""
Review Tools - å·¥å…·å‡½æ•°é›†
ä¸º Reviewer Agents æä¾›çš„ä¸“ä¸šå·¥å…·å‡½æ•°

è¿™äº›å·¥å…·å‡½æ•°ä¼šè¢« Reviewer Agents è°ƒç”¨ï¼Œè€Œä¸æ˜¯ç›´æ¥ç”± Coordinator è°ƒç”¨
"""

import json
import logging
from typing import List, Dict, Any
from datetime import datetime

from utils.mcp_client import XiaohongshuMCPClient
from utils.response_utils import create_success_response, create_error_response

logger = logging.getLogger(__name__)


# ========== Engagement Reviewer å·¥å…· ==========

def search_similar_posts(topic: str, limit: int = 5, min_likes: int = 1000) -> str:
    """
    æœç´¢ç±»ä¼¼è¯é¢˜çš„çˆ†æ¬¾å¸–å­
    
    è¿™ä¸ªå·¥å…·å¸®åŠ© Engagement Reviewer Agent åˆ†æåŒç±»å†…å®¹çš„è¡¨ç°ã€‚
    
    Args:
        topic: è¯é¢˜å…³é”®è¯
        limit: è¿”å›æ•°é‡
        min_likes: æœ€ä½ç‚¹èµé‡
        
    Returns:
        JSON æ ¼å¼çš„å¸–å­åˆ—è¡¨
        
    Example (Agent å¦‚ä½•è°ƒç”¨):
        >>> # Agent ä¼šè¿™æ ·è°ƒç”¨ï¼š
        >>> result = search_similar_posts("æ‚‰å°¼æ—…æ¸¸", limit=5)
        >>> posts = json.loads(result)
        >>> # Agent å¯ä»¥åˆ†æè¿™äº›çˆ†æ¬¾å¸–å­çš„ç‰¹å¾
    """
    try:
        logger.info(f"æœç´¢çˆ†æ¬¾å¸–å­: topic={topic}, limit={limit}")
        
        # è°ƒç”¨ MCP æœç´¢
        mcp_client = XiaohongshuMCPClient()
        search_result = mcp_client.search_notes(
            keyword=topic,
            limit=limit * 2  # å¤šæœç´¢ä¸€äº›ï¼Œç„¶åè¿‡æ»¤
        )
        
        # è§£æç»“æœ
        # search_notes ç›´æ¥è¿”å› dictï¼Œä¸éœ€è¦ json.loads
        # è¿”å›æ ¼å¼: {'feeds': [...], 'count': N}
        posts = search_result.get('feeds', [])
        
        # è¿‡æ»¤ï¼šåªä¿ç•™ç‚¹èµé‡é«˜çš„
        # æ³¨æ„ï¼šMCP è¿”å›çš„å­—æ®µåå¯èƒ½æ˜¯ liked_count è€Œä¸æ˜¯ likes
        hot_posts = [
            p for p in posts 
            if p.get('liked_count', p.get('likes', 0)) >= min_likes
        ][:limit]
        
        # æ ¼å¼åŒ–è¿”å›
        formatted_posts = []
        for post in hot_posts:
            formatted_posts.append({
                "title": post.get('title', ''),
                "likes": post.get('liked_count', post.get('likes', 0)),
                "comments": post.get('comment_count', post.get('comments', 0)),
                "favorites": post.get('collected_count', post.get('favorites', 0)),
                "content_preview": post.get('desc', post.get('content', ''))[:100]
            })
        
        return create_success_response(
            data={
                "topic": topic,
                "count": len(formatted_posts),
                "posts": formatted_posts
            },
            message=f"æ‰¾åˆ° {len(formatted_posts)} ç¯‡çˆ†æ¬¾å¸–å­"
        )
        
    except Exception as e:
        logger.error(f"æœç´¢çˆ†æ¬¾å¸–å­å¤±è´¥: {str(e)}", exc_info=True)
        return create_error_response(f"æœç´¢å¤±è´¥: {str(e)}")


def analyze_title_patterns(titles: List[str]) -> str:
    """
    åˆ†ææ ‡é¢˜è§„å¾‹
    
    è¯†åˆ«æ ‡é¢˜ä¸­çš„å¸¸è§æ¨¡å¼ï¼šæ•°å­—ã€ç–‘é—®ã€æƒ…æ„Ÿè¯ã€ç¬¦å·ç­‰ã€‚
    
    Args:
        titles: æ ‡é¢˜åˆ—è¡¨ï¼ˆé€šå¸¸æ¥è‡ª search_similar_postsï¼‰
        
    Returns:
        JSON æ ¼å¼çš„åˆ†æç»“æœ
        
    Example (Agent å¦‚ä½•è°ƒç”¨):
        >>> # Agent å…ˆæœç´¢
        >>> posts = search_similar_posts("æ‚‰å°¼æ—…æ¸¸")
        >>> titles = [p['title'] for p in posts['data']['posts']]
        >>> # ç„¶ååˆ†ææ ‡é¢˜è§„å¾‹
        >>> patterns = analyze_title_patterns(titles)
    """
    try:
        import re
        
        # å¤„ç†ç©ºè¾“å…¥æˆ–å­—ç¬¦ä¸²è¾“å…¥ï¼ˆAgent å¯èƒ½ä¼ é”™ï¼‰
        if not titles:
            return create_error_response("æ ‡é¢˜åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•åˆ†æ")
        
        # å¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢
        if isinstance(titles, str):
            if not titles.strip():
                return create_error_response("æ ‡é¢˜åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•åˆ†æ")
            # å‡è®¾æ˜¯å•ä¸ªæ ‡é¢˜
            titles = [titles]
        
        # ç»Ÿè®¡å„ç§æ¨¡å¼
        has_numbers = sum(1 for t in titles if re.search(r'\d+', t))
        has_question = sum(1 for t in titles if '?' in t or 'å—' in t)
        has_emoji = sum(1 for t in titles if any(c for c in t if ord(c) > 127462))
        has_exclamation = sum(1 for t in titles if '!' in t or 'ï¼' in t)
        
        # å¸¸è§æƒ…æ„Ÿè¯
        emotion_words = ['ç»äº†', 'å¤ªçˆ±äº†', 'æƒŠå–œ', 'å¿…å»', 'æ¨è', 'å€¼å¾—', 'éœ‡æ’¼', 'ç¾å“­']
        has_emotion = sum(
            1 for t in titles 
            if any(word in t for word in emotion_words)
        )
        
        total = len(titles)
        
        # å†æ¬¡æ£€æŸ¥ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
        if total == 0:
            return create_error_response("æ ‡é¢˜åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•åˆ†æ")
        
        # è®¡ç®—å æ¯”
        patterns = {
            "numbers": {
                "count": has_numbers,
                "percentage": round(has_numbers / total * 100, 1) if total > 0 else 0,
                "example": "3å¤©2å¤œã€10ä¸ªå¿…å»æ™¯ç‚¹"
            },
            "question": {
                "count": has_question,
                "percentage": round(has_question / total * 100, 1) if total > 0 else 0,
                "example": "ä½ çŸ¥é“å—ï¼Ÿæ€ä¹ˆåŠï¼Ÿ"
            },
            "emoji": {
                "count": has_emoji,
                "percentage": round(has_emoji / total * 100, 1) if total > 0 else 0,
                "example": "ğŸ˜Šâ¤ï¸âœ¨"
            },
            "exclamation": {
                "count": has_exclamation,
                "percentage": round(has_exclamation / total * 100, 1) if total > 0 else 0,
                "example": "å¤ªæ£’äº†ï¼ç»äº†ï¼"
            },
            "emotion_words": {
                "count": has_emotion,
                "percentage": round(has_emotion / total * 100, 1) if total > 0 else 0,
                "example": "ç»äº†ã€å¤ªçˆ±äº†ã€æ¨è"
            }
        }
        
        # è¯†åˆ«å¸¸è§çš„æ ‡é¢˜ç»“æ„
        structures = []
        if has_numbers / total > 0.5:
            structures.append("æ•°å­—åŒ–æ ‡é¢˜å¾ˆå—æ¬¢è¿")
        if has_question / total > 0.3:
            structures.append("ç–‘é—®å¼æ ‡é¢˜èƒ½æ¿€å‘å¥½å¥‡")
        if has_emotion / total > 0.4:
            structures.append("æƒ…æ„Ÿè¯æ±‡æå‡å¸å¼•åŠ›")
        
        return create_success_response(
            data={
                "total_analyzed": total,
                "patterns": patterns,
                "insights": structures,
                "recommendation": "æ ‡é¢˜åº”åŒ…å«æ•°å­—ã€æƒ…æ„Ÿè¯å’Œé€‚å½“çš„ç¬¦å·"
            },
            message=f"åˆ†æäº† {total} ä¸ªæ ‡é¢˜"
        )
        
    except Exception as e:
        logger.error(f"æ ‡é¢˜åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
        return create_error_response(f"åˆ†æå¤±è´¥: {str(e)}")


def check_emotional_triggers(content: str) -> str:
    """
    æ£€æŸ¥æƒ…æ„Ÿè§¦å‘ç‚¹
    
    è¯„ä¼°å†…å®¹æ˜¯å¦èƒ½è§¦å‘æƒ…æ„Ÿå…±é¸£ã€å¥½å¥‡å¿ƒã€å®ç”¨ä»·å€¼ç­‰ã€‚
    
    Args:
        content: å¸–å­å†…å®¹
        
    Returns:
        JSON æ ¼å¼çš„æƒ…æ„Ÿè§¦å‘åˆ†æ
    """
    try:
        # æƒ…æ„Ÿå…³é”®è¯å­—å…¸
        triggers = {
            "å…±é¸£": ["æˆ‘ä¹Ÿæ˜¯", "å¤ªçœŸå®äº†", "æ„ŸåŒèº«å—", "è¯´åˆ°å¿ƒå", "æ·±æœ‰ä½“ä¼š"],
            "å¥½å¥‡": ["åŸæ¥", "ç«Ÿç„¶", "æ²¡æƒ³åˆ°", "å‘ç°", "ç§˜å¯†", "çœŸç›¸"],
            "å®ç”¨": ["æ–¹æ³•", "æŠ€å·§", "æ”»ç•¥", "æ•™ç¨‹", "æ­¥éª¤", "æŒ‡å—"],
            "æƒŠå–œ": ["æ„å¤–", "è¶…å‡ºé¢„æœŸ", "æƒŠè‰³", "éœ‡æ’¼", "å¤ªæ£’äº†"],
            "äº‰è®®": ["ä½†æ˜¯", "å…¶å®", "ä¸è¿‡", "ç›¸å", "æ‰“è„¸"]
        }
        
        # æ£€æµ‹æ¯ç§è§¦å‘ç‚¹
        detected = {}
        for trigger_type, keywords in triggers.items():
            found_keywords = [kw for kw in keywords if kw in content]
            detected[trigger_type] = {
                "found": len(found_keywords) > 0,
                "keywords": found_keywords,
                "count": len(found_keywords)
            }
        
        # è®¡ç®—è§¦å‘å¼ºåº¦
        total_triggers = sum(t['count'] for t in detected.values())
        strength = "å¼º" if total_triggers >= 5 else "ä¸­" if total_triggers >= 3 else "å¼±"
        
        # ç”Ÿæˆå»ºè®®
        suggestions = []
        if not detected["å…±é¸£"]["found"]:
            suggestions.append("å¢åŠ èƒ½å¼•å‘å…±é¸£çš„è¡¨è¾¾")
        if not detected["å®ç”¨"]["found"]:
            suggestions.append("å¼ºè°ƒå®ç”¨ä»·å€¼")
        
        return create_success_response(
            data={
                "triggers": detected,
                "total_count": total_triggers,
                "strength": strength,
                "suggestions": suggestions
            },
            message=f"æ£€æµ‹åˆ° {total_triggers} ä¸ªæƒ…æ„Ÿè§¦å‘ç‚¹ï¼Œå¼ºåº¦: {strength}"
        )
        
    except Exception as e:
        logger.error(f"æƒ…æ„Ÿè§¦å‘æ£€æµ‹å¤±è´¥: {str(e)}", exc_info=True)
        return create_error_response(f"æ£€æµ‹å¤±è´¥: {str(e)}")


def get_engagement_stats(topic: str) -> str:
    """
    è·å–åŒç±»å†…å®¹çš„å¹³å‡äº’åŠ¨æ•°æ®
    
    å¸®åŠ© Agent äº†è§£è¯¥è¯é¢˜çš„æ­£å¸¸äº’åŠ¨æ°´å¹³ã€‚
    
    Args:
        topic: è¯é¢˜
        
    Returns:
        JSON æ ¼å¼çš„ç»Ÿè®¡æ•°æ®
    """
    try:
        # æœç´¢ç›¸å…³å¸–å­
        search_result = search_similar_posts(topic, limit=20, min_likes=0)
        search_data = json.loads(search_result)
        
        if not search_data.get('success'):
            return create_error_response("æ— æ³•è·å–ç»Ÿè®¡æ•°æ®")
        
        posts = search_data.get('data', {}).get('posts', [])
        
        if not posts:
            return create_error_response("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å¸–å­")
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total = len(posts)
        avg_likes = sum(p['likes'] for p in posts) / total
        avg_comments = sum(p['comments'] for p in posts) / total
        avg_favorites = sum(p.get('favorites', 0) for p in posts) / total
        
        # æ‰¾å‡ºè¡¨ç°æœ€å¥½çš„
        top_post = max(posts, key=lambda p: p['likes'])
        
        return create_success_response(
            data={
                "topic": topic,
                "sample_size": total,
                "averages": {
                    "likes": round(avg_likes, 0),
                    "comments": round(avg_comments, 0),
                    "favorites": round(avg_favorites, 0)
                },
                "top_performer": {
                    "title": top_post['title'],
                    "likes": top_post['likes'],
                    "comments": top_post['comments']
                },
                "benchmark": {
                    "good": f"ç‚¹èµ > {int(avg_likes)}",
                    "excellent": f"ç‚¹èµ > {int(avg_likes * 2)}"
                }
            },
            message=f"åŸºäº {total} ç¯‡å¸–å­çš„ç»Ÿè®¡"
        )
        
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        return create_error_response(f"è·å–å¤±è´¥: {str(e)}")


# ========== Quality Reviewer å·¥å…· ==========

def check_readability(content: str) -> str:
    """
    æ£€æŸ¥å¯è¯»æ€§
    
    è¯„ä¼°å†…å®¹çš„é˜…è¯»ä½“éªŒï¼šå¥å­é•¿åº¦ã€ä¸“ä¸šæœ¯è¯­ã€æ’ç‰ˆç­‰ã€‚
    
    Args:
        content: å¸–å­å†…å®¹
        
    Returns:
        JSON æ ¼å¼çš„å¯è¯»æ€§åˆ†æ
    """
    try:
        # åˆ†æå¥å­é•¿åº¦
        sentences = content.replace('ï¼', 'ã€‚').replace('ï¼Ÿ', 'ã€‚').split('ã€‚')
        sentences = [s.strip() for s in sentences if s.strip()]
        
        avg_sentence_length = sum(len(s) for s in sentences) / len(sentences) if sentences else 0
        long_sentences = sum(1 for s in sentences if len(s) > 50)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ®µè½
        paragraphs = content.split('\n')
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        # æ£€æŸ¥ä¸“ä¸šæœ¯è¯­ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # å®é™…åº”è¯¥æœ‰ä¸“ä¸šæœ¯è¯­è¯å…¸
        has_complex_words = any(
            word in content 
            for word in ['å› æ­¤', 'ç„¶è€Œ', 'é‰´äº', 'ç»¼ä¸Šæ‰€è¿°']
        )
        
        # æ£€æŸ¥æ’ç‰ˆå…ƒç´ 
        has_emoji = any(ord(c) > 127462 for c in content)
        has_line_breaks = '\n' in content.strip()
        has_bullet_points = 'â€¢' in content or 'Â·' in content or '-' in content
        
        # è®¡ç®—å¯è¯»æ€§è¯„åˆ†
        readability_score = 10
        
        if avg_sentence_length > 40:
            readability_score -= 2
        elif avg_sentence_length > 30:
            readability_score -= 1
            
        if long_sentences > len(sentences) * 0.3:  # è¶…è¿‡30%çš„å¥å­è¿‡é•¿
            readability_score -= 1
            
        if not has_line_breaks and len(content) > 200:
            readability_score -= 2
            
        if not has_emoji:
            readability_score -= 0.5
            
        # ç”Ÿæˆå»ºè®®
        suggestions = []
        if avg_sentence_length > 35:
            suggestions.append("å¥å­å¹³å‡é•¿åº¦è¾ƒé•¿ï¼Œå»ºè®®æ‹†åˆ†ä¸ºçŸ­å¥")
        if not has_line_breaks:
            suggestions.append("å»ºè®®ä½¿ç”¨åˆ†è¡Œæå‡å¯è¯»æ€§")
        if not has_emoji and not has_bullet_points:
            suggestions.append("é€‚å½“æ·»åŠ emojiæˆ–ç¬¦å·ç‚¹ç¼€")
        if long_sentences > 3:
            suggestions.append(f"æœ‰ {long_sentences} ä¸ªå¥å­è¿‡é•¿ï¼Œå»ºè®®ç®€åŒ–")
            
        return create_success_response(
            data={
                "score": round(readability_score, 1),
                "metrics": {
                    "avg_sentence_length": round(avg_sentence_length, 1),
                    "long_sentences_count": long_sentences,
                    "paragraph_count": len(paragraphs),
                    "has_emoji": has_emoji,
                    "has_formatting": has_line_breaks or has_bullet_points
                },
                "reading_level": "æ˜“è¯»" if readability_score >= 8 else "ä¸€èˆ¬" if readability_score >= 6 else "è¾ƒéš¾",
                "suggestions": suggestions
            },
            message=f"å¯è¯»æ€§è¯„åˆ†: {readability_score:.1f}/10"
        )
        
    except Exception as e:
        logger.error(f"å¯è¯»æ€§æ£€æŸ¥å¤±è´¥: {str(e)}", exc_info=True)
        return create_error_response(f"æ£€æŸ¥å¤±è´¥: {str(e)}")


def analyze_content_depth(content: str, topic: str = "") -> str:
    """
    åˆ†æå†…å®¹æ·±åº¦
    
    è¯„ä¼°å†…å®¹çš„ä¿¡æ¯é‡ã€ç‹¬ç‰¹æ€§å’Œä»·å€¼ã€‚
    
    Args:
        content: å¸–å­å†…å®¹
        topic: è¯é¢˜ï¼ˆç”¨äºåˆ¤æ–­ç›¸å…³æ€§ï¼‰
        
    Returns:
        JSON æ ¼å¼çš„æ·±åº¦åˆ†æ
    """
    try:
        # 1. ä¿¡æ¯å¯†åº¦
        word_count = len(content)
        
        # 2. æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“ä¿¡æ¯
        has_numbers = bool(__import__('re').search(r'\d+', content))
        has_specific_names = bool(__import__('re').search(r'[A-Z][a-z]+|[\u4e00-\u9fa5]{2,}(?:åº—|é¦†|ä¸­å¿ƒ|å…¬å›­|é…’åº—)', content))
        
        # 3. æ£€æŸ¥æ˜¯å¦æœ‰ä¸ªäººè§è§£
        opinion_markers = ['æˆ‘è§‰å¾—', 'æˆ‘è®¤ä¸º', 'åœ¨æˆ‘çœ‹æ¥', 'ä¸ªäºº', 'æ¨è', 'å»ºè®®']
        has_personal_view = any(marker in content for marker in opinion_markers)
        
        # 4. æ£€æŸ¥æ˜¯å¦æœ‰æ¡ˆä¾‹æˆ–ä¾‹å­
        example_markers = ['æ¯”å¦‚', 'ä¾‹å¦‚', 'ä¸¾ä¸ªä¾‹å­', 'ä»¥æˆ‘', 'æˆ‘çš„']
        has_examples = any(marker in content for marker in example_markers)
        
        # 5. æ£€æŸ¥æ˜¯å¦æœ‰å®ç”¨å»ºè®®
        practical_markers = ['æ–¹æ³•', 'æ­¥éª¤', 'æŠ€å·§', 'æ”»ç•¥', 'æ³¨æ„', 'è®°å¾—', 'åƒä¸‡']
        has_practical_info = any(marker in content for marker in practical_markers)
        
        # è®¡ç®—æ·±åº¦è¯„åˆ†
        depth_score = 5  # åŸºç¡€åˆ†
        
        if word_count >= 500:
            depth_score += 2
        elif word_count >= 300:
            depth_score += 1
        elif word_count < 150:
            depth_score -= 1
            
        if has_numbers:
            depth_score += 0.5
        if has_specific_names:
            depth_score += 0.5
        if has_personal_view:
            depth_score += 1
        if has_examples:
            depth_score += 0.5
        if has_practical_info:
            depth_score += 0.5
            
        depth_score = min(10, depth_score)  # æœ€é«˜10åˆ†
        
        # ç”Ÿæˆå»ºè®®
        suggestions = []
        if word_count < 200:
            suggestions.append("å†…å®¹è¾ƒçŸ­ï¼Œå»ºè®®æ‰©å……åˆ°300å­—ä»¥ä¸Š")
        if not has_numbers:
            suggestions.append("æ·»åŠ å…·ä½“æ•°å­—å¢å¼ºå¯ä¿¡åº¦")
        if not has_personal_view:
            suggestions.append("å¢åŠ ä¸ªäººè§è§£å’Œæ„Ÿå—")
        if not has_examples:
            suggestions.append("åŠ å…¥å…·ä½“æ¡ˆä¾‹æˆ–ä¾‹å­")
        if not has_practical_info:
            suggestions.append("æä¾›æ›´å¤šå®ç”¨å»ºè®®")
            
        return create_success_response(
            data={
                "score": round(depth_score, 1),
                "metrics": {
                    "word_count": word_count,
                    "has_numbers": has_numbers,
                    "has_specific_info": has_specific_names,
                    "has_personal_view": has_personal_view,
                    "has_examples": has_examples,
                    "has_practical_info": has_practical_info
                },
                "depth_level": "æ·±å…¥" if depth_score >= 8 else "ä¸­ç­‰" if depth_score >= 6 else "æµ…æ˜¾",
                "suggestions": suggestions
            },
            message=f"å†…å®¹æ·±åº¦è¯„åˆ†: {depth_score:.1f}/10"
        )
        
    except Exception as e:
        logger.error(f"å†…å®¹æ·±åº¦åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
        return create_error_response(f"åˆ†æå¤±è´¥: {str(e)}")


def check_information_accuracy(content: str, topic: str = "") -> str:
    """
    æ£€æŸ¥ä¿¡æ¯å‡†ç¡®æ€§
    
    æ£€æµ‹æ˜æ˜¾çš„äº‹å®é”™è¯¯ã€ä¸åˆç†çš„æ•°æ®ã€è¯¯å¯¼æ€§è¡¨è¿°ã€‚
    
    Args:
        content: å¸–å­å†…å®¹
        topic: è¯é¢˜ï¼ˆç”¨äºä¸Šä¸‹æ–‡åˆ¤æ–­ï¼‰
        
    Returns:
        JSON æ ¼å¼çš„å‡†ç¡®æ€§æ£€æŸ¥ç»“æœ
    """
    try:
        issues = []
        
        # 1. æ£€æŸ¥æé™è¯ï¼ˆå¯èƒ½è¿åå¹¿å‘Šæ³•ï¼‰
        extreme_words = ['æœ€å¥½', 'ç¬¬ä¸€', 'æœ€å¤§', 'æœ€å¼º', 'é¡¶çº§', 'æè‡´', 'å®Œç¾']
        found_extreme = [w for w in extreme_words if w in content]
        if found_extreme:
            issues.append({
                "type": "æé™è¯",
                "issue": f"ä½¿ç”¨äº†æé™è¯: {', '.join(found_extreme)}",
                "severity": "medium",
                "suggestion": "æ›¿æ¢ä¸ºç›¸å¯¹è¡¨è¿°ï¼Œå¦‚'éå¸¸å¥½'ã€'å¾ˆæ¨è'"
            })
        
        # 2. æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾å¤¸å¼ çš„æ•°å­—
        import re
        numbers = re.findall(r'\d+', content)
        for num in numbers:
            if int(num) > 10000:  # ç®€å•æ£€æŸ¥
                # è¿™é‡Œå®é™…åº”è¯¥æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­
                pass
        
        # 3. æ£€æŸ¥æ˜¯å¦æœ‰ç»å¯¹åŒ–è¡¨è¿°
        absolute_words = ['ä¸€å®š', 'å¿…é¡»', 'ç»å¯¹', 'ç™¾åˆ†ä¹‹ç™¾', 'ä¿è¯']
        found_absolute = [w for w in absolute_words if w in content]
        if found_absolute:
            issues.append({
                "type": "ç»å¯¹åŒ–è¡¨è¿°",
                "issue": f"ä½¿ç”¨äº†ç»å¯¹åŒ–è¡¨è¿°: {', '.join(found_absolute)}",
                "severity": "low",
                "suggestion": "ä½¿ç”¨æ›´æ¸©å’Œçš„è¡¨è¿°ï¼Œå¦‚'å»ºè®®'ã€'æ¨è'"
            })
        
        # 4. æ£€æŸ¥æ˜¯å¦æœ‰æœªç»è¯å®çš„å®£ç§°
        claim_words = ['åŒ…æ²»', 'æ ¹æ²»', 'å½»åº•', 'æ°¸ä¹…', 'ç§˜æ–¹']
        found_claims = [w for w in claim_words if w in content]
        if found_claims:
            issues.append({
                "type": "å¤¸å¤§å®£ç§°",
                "issue": f"å¯èƒ½å­˜åœ¨å¤¸å¤§å®£ç§°: {', '.join(found_claims)}",
                "severity": "high",
                "suggestion": "åˆ é™¤æˆ–ä¿®æ”¹ä¸ºå®¢è§‚è¡¨è¿°"
            })
        
        # è®¡ç®—å‡†ç¡®æ€§è¯„åˆ†
        accuracy_score = 10
        for issue in issues:
            if issue['severity'] == 'high':
                accuracy_score -= 3
            elif issue['severity'] == 'medium':
                accuracy_score -= 1.5
            elif issue['severity'] == 'low':
                accuracy_score -= 0.5
        
        accuracy_score = max(0, accuracy_score)
        
        return create_success_response(
            data={
                "score": round(accuracy_score, 1),
                "issues": issues,
                "total_issues": len(issues),
                "risk_level": "é«˜" if accuracy_score < 6 else "ä¸­" if accuracy_score < 8 else "ä½",
                "passed": len([i for i in issues if i['severity'] == 'high']) == 0
            },
            message=f"å‘ç° {len(issues)} ä¸ªå‡†ç¡®æ€§é—®é¢˜" if issues else "æœªå‘ç°æ˜æ˜¾é—®é¢˜"
        )
        
    except Exception as e:
        logger.error(f"å‡†ç¡®æ€§æ£€æŸ¥å¤±è´¥: {str(e)}", exc_info=True)
        return create_error_response(f"æ£€æŸ¥å¤±è´¥: {str(e)}")


def check_grammar(text: str) -> str:
    """
    è¯­æ³•æ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
    
    æ£€æŸ¥åŸºæœ¬çš„è¯­æ³•é—®é¢˜ï¼šæ ‡ç‚¹ã€æ‹¼å†™ã€é‡å¤è¯ç­‰ã€‚
    
    Args:
        text: å¾…æ£€æŸ¥çš„æ–‡æœ¬
        
    Returns:
        JSON æ ¼å¼çš„è¯­æ³•é—®é¢˜åˆ—è¡¨
    """
    try:
        issues = []
        
        # æ£€æŸ¥ 1: æ ‡ç‚¹ç¬¦å·
        if text.count('ã€‚') + text.count('ï¼') + text.count('ï¼Ÿ') == 0:
            issues.append({
                "type": "æ ‡ç‚¹",
                "issue": "ç¼ºå°‘å¥å·æˆ–æ„Ÿå¹å·",
                "severity": "medium"
            })
        
        # æ£€æŸ¥ 2: é‡å¤è¯
        words = text.split()
        for i in range(len(words) - 1):
            if words[i] == words[i + 1] and len(words[i]) > 1:
                issues.append({
                    "type": "é‡å¤",
                    "issue": f"é‡å¤è¯: {words[i]}",
                    "severity": "low"
                })
        
        # æ£€æŸ¥ 3: å¸¸è§æ‹¼å†™é”™è¯¯ï¼ˆç®€åŒ–ï¼‰
        common_typos = {
            'çš„åœ°å¾—': 'çš„/åœ°/å¾— æ··ç”¨',
            'åœ¨å†': 'åœ¨/å† æ··ç”¨'
        }
        for typo_pair, desc in common_typos.items():
            # ç®€åŒ–æ£€æŸ¥é€»è¾‘
            pass
        
        return create_success_response(
            data={
                "total_issues": len(issues),
                "issues": issues,
                "score": 10 - len(issues) * 0.5  # æ¯ä¸ªé—®é¢˜æ‰£ 0.5 åˆ†
            },
            message=f"å‘ç° {len(issues)} ä¸ªè¯­æ³•é—®é¢˜"
        )
        
    except Exception as e:
        logger.error(f"è¯­æ³•æ£€æŸ¥å¤±è´¥: {str(e)}", exc_info=True)
        return create_error_response(f"æ£€æŸ¥å¤±è´¥: {str(e)}")


def analyze_content_structure(content: str) -> str:
    """
    åˆ†æå†…å®¹ç»“æ„
    
    æ£€æŸ¥æ˜¯å¦æœ‰æ¸…æ™°çš„å¼€å¤´ã€æ­£æ–‡ã€ç»“å°¾ã€‚
    
    Args:
        content: å¸–å­å†…å®¹
        
    Returns:
        JSON æ ¼å¼çš„ç»“æ„åˆ†æ
    """
    try:
        lines = content.strip().split('\n')
        total_lines = len(lines)
        
        # ç®€åŒ–çš„ç»“æ„åˆ†æ
        has_intro = len(lines[0]) < 100 if total_lines > 0 else False  # å¼€å¤´è¾ƒçŸ­
        has_body = total_lines >= 3  # è‡³å°‘3æ®µ
        has_ending = 'æ€»ç»“' in content or 'æœ€å' in content or 'è®°å¾—' in content
        
        structure_score = 0
        if has_intro:
            structure_score += 3
        if has_body:
            structure_score += 4
        if has_ending:
            structure_score += 3
        
        suggestions = []
        if not has_intro:
            suggestions.append("å»ºè®®æ·»åŠ å¼•äººå…¥èƒœçš„å¼€å¤´")
        if not has_body:
            suggestions.append("å†…å®¹è¿‡äºç®€çŸ­ï¼Œå»ºè®®æ‰©å……")
        if not has_ending:
            suggestions.append("å»ºè®®æ·»åŠ æ€»ç»“æˆ–è¡ŒåŠ¨å·å¬")
        
        return create_success_response(
            data={
                "structure": {
                    "has_intro": has_intro,
                    "has_body": has_body,
                    "has_ending": has_ending
                },
                "paragraph_count": total_lines,
                "score": structure_score,
                "suggestions": suggestions
            },
            message=f"ç»“æ„è¯„åˆ†: {structure_score}/10"
        )
        
    except Exception as e:
        logger.error(f"ç»“æ„åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
        return create_error_response(f"åˆ†æå¤±è´¥: {str(e)}")


# ========== Compliance Reviewer å·¥å…· ==========

def check_sensitive_words_detailed(text: str) -> str:
    """
    è¯¦ç»†çš„æ•æ„Ÿè¯æ£€æµ‹
    
    æ¯” review_tools_v1.py ä¸­çš„ç‰ˆæœ¬æ›´è¯¦ç»†ã€‚
    
    Args:
        text: å¾…æ£€æŸ¥çš„æ–‡æœ¬
        
    Returns:
        JSON æ ¼å¼çš„æ£€æµ‹ç»“æœ
    """
    try:
        # æ‰©å±•çš„æ•æ„Ÿè¯åº“ï¼ˆå®é™…åº”è¯¥æ›´å®Œæ•´ï¼‰
        sensitive_categories = {
            "æ”¿æ²»æ•æ„Ÿ": ['æ”¿æ²»', 'æ”¿åºœ', 'é¢†å¯¼äºº'],
            "è¿æ³•è¿è§„": ['èµŒåš', 'è‰²æƒ…', 'æ¯’å“', 'é»„èµŒæ¯’', 'è¯ˆéª—'],
            "æš´åŠ›ææ€–": ['æš´åŠ›', 'ææ€–', 'è¡€è…¥', 'æ€äºº'],
            "è¿·ä¿¡å®—æ•™": ['é‚ªæ•™', 'è¿·ä¿¡', 'ç®—å‘½']
        }
        
        detected = {}
        total_issues = 0
        
        for category, words in sensitive_categories.items():
            found = [w for w in words if w in text]
            if found:
                detected[category] = found
                total_issues += len(found)
        
        risk_level = "high" if total_issues > 0 else "low"
        
        return create_success_response(
            data={
                "detected": detected,
                "total_issues": total_issues,
                "risk_level": risk_level,
                "passed": total_issues == 0
            },
            message=f"æ£€æµ‹åˆ° {total_issues} ä¸ªæ•æ„Ÿè¯" if total_issues > 0 else "æœªæ£€æµ‹åˆ°æ•æ„Ÿè¯"
        )
        
    except Exception as e:
        logger.error(f"æ•æ„Ÿè¯æ£€æµ‹å¤±è´¥: {str(e)}", exc_info=True)
        return create_error_response(f"æ£€æµ‹å¤±è´¥: {str(e)}")


def query_platform_rules(content_type: str = "image_post") -> str:
    """
    æŸ¥è¯¢å°çº¢ä¹¦å¹³å°è§„åˆ™
    
    è¿”å›ç‰¹å®šç±»å‹å†…å®¹çš„å¹³å°è§„åˆ™ã€‚
    
    Args:
        content_type: å†…å®¹ç±»å‹ï¼ˆimage_post, video, liveç­‰ï¼‰
        
    Returns:
        JSON æ ¼å¼çš„å¹³å°è§„åˆ™
    """
    try:
        # æ¨¡æ‹Ÿè§„åˆ™åº“ï¼ˆå®é™…åº”è¯¥ä»æ•°æ®åº“æˆ–é…ç½®æ–‡ä»¶è¯»å–ï¼‰
        rules = {
            "image_post": {
                "title_max_length": 20,
                "content_max_length": 1000,
                "images_min": 1,
                "images_max": 9,
                "tags_min": 3,
                "tags_max": 10,
                "forbidden_content": [
                    "ä¸å¾—åŒ…å«è”ç³»æ–¹å¼ï¼ˆå¾®ä¿¡ã€QQç­‰ï¼‰",
                    "ä¸å¾—ä½¿ç”¨æé™è¯ï¼ˆæœ€å¥½ã€ç¬¬ä¸€ç­‰ï¼‰",
                    "ä¸å¾—è™šå‡å®£ä¼ ",
                    "ä¸å¾—ä¾µçŠ¯ç‰ˆæƒ"
                ]
            },
            "video": {
                "duration_min": 3,
                "duration_max": 300,
                "title_max_length": 20,
                "forbidden_content": [
                    "ä¸å¾—åŒ…å«æ°´å°",
                    "ä¸å¾—æ¬è¿ä»–äººä½œå“"
                ]
            }
        }
        
        rule = rules.get(content_type, rules["image_post"])
        
        return create_success_response(
            data={
                "content_type": content_type,
                "rules": rule,
                "last_updated": "2025-11-01"
            },
            message=f"å·²è·å– {content_type} çš„å¹³å°è§„åˆ™"
        )
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¹³å°è§„åˆ™å¤±è´¥: {str(e)}", exc_info=True)
        return create_error_response(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


# ========== å¯¼å‡ºæ‰€æœ‰å·¥å…· ==========

__all__ = [
    # Engagement Reviewer å·¥å…·
    "search_similar_posts",
    "analyze_title_patterns",
    "check_emotional_triggers",
    "get_engagement_stats",
    
    # Quality Reviewer å·¥å…·
    "check_readability",
    "analyze_content_depth",
    "check_information_accuracy",
    "check_grammar",
    "analyze_content_structure",
    
    # Compliance Reviewer å·¥å…·
    "check_sensitive_words_detailed",
    "query_platform_rules"
]
