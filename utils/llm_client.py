"""
LLM Client - ç»Ÿä¸€çš„ LLM è°ƒç”¨å°è£…
æ”¯æŒ OpenAIã€Anthropicã€Ollama ç­‰å¤šç§æä¾›å•†
"""

import logging
import os
from typing import Optional, List, Dict, Any
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

# å°è¯•å¯¼å…¥æ‰€éœ€çš„åº“
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

try:
    from anthropic import Anthropic
except ImportError:
    Anthropic = None

from config import ModelConfig

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class LLMError(Exception):
    """LLM è°ƒç”¨å¼‚å¸¸"""
    pass


class LLMClient:
    """
    ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯å°è£…
    
    æ”¯æŒå¤šä¸ªæä¾›å•†ï¼š
    - OpenAI (gpt-4o, gpt-4o-mini, gpt-4o-vision)
    - Anthropic (claude-3.5-sonnet)
    - Ollama (llama3.2, åŠå…¶ä»–æœ¬åœ°æ¨¡å‹)
    - ç¬¬ä¸‰æ–¹ OpenAI å…¼å®¹å¹³å°
    """
    
    def __init__(
        self,
        openai_api_key: Optional[str] = None,
        openai_base_url: Optional[str] = None,
        anthropic_api_key: Optional[str] = None,
        ollama_base_url: Optional[str] = None
    ):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        
        Args:
            openai_api_key: OpenAI API Keyï¼ˆå¦‚æœä½¿ç”¨ OpenAI æˆ–ç¬¬ä¸‰æ–¹å¹³å°ï¼‰
            openai_base_url: OpenAI Base URLï¼ˆå¯é€‰ï¼Œç”¨äºç¬¬ä¸‰æ–¹å¹³å°ï¼‰
            anthropic_api_key: Anthropic API Keyï¼ˆå¦‚æœä½¿ç”¨ Claudeï¼‰
            ollama_base_url: Ollama Base URLï¼ˆé»˜è®¤ï¼šhttp://localhost:11434/v1ï¼‰
        """
        # ä»é…ç½®æˆ–å‚æ•°è·å– API Keys
        self.openai_api_key = openai_api_key or ModelConfig.OPENAI_API_KEY
        self.openai_base_url = openai_base_url or ModelConfig.OPENAI_BASE_URL
        self.anthropic_api_key = anthropic_api_key or ModelConfig.ANTHROPIC_API_KEY
        self.ollama_base_url = ollama_base_url or ModelConfig.OLLAMA_BASE_URL
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._openai_client = None
        self._anthropic_client = None
        
        # æ£€æŸ¥å¿…è¦çš„åº“æ˜¯å¦å·²å®‰è£…
        if OpenAI is None:
            logger.warning("openai åº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ OpenAI æ¨¡å‹")
        if Anthropic is None:
            logger.warning("anthropic åº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ Claude æ¨¡å‹")
    
    def _get_openai_client(self) -> Optional[OpenAI]:
        """è·å– OpenAI å®¢æˆ·ç«¯ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if OpenAI is None:
            return None
        
        if self._openai_client is None:
            if not self.openai_api_key:
                return None
            
            kwargs = {"api_key": self.openai_api_key}
            if self.openai_base_url:
                kwargs["base_url"] = self.openai_base_url
            
            self._openai_client = OpenAI(**kwargs)
            logger.debug(f"åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ŒBase URL: {self.openai_base_url or 'é»˜è®¤'}")
        
        return self._openai_client
    
    def _get_anthropic_client(self) -> Optional[Anthropic]:
        """è·å– Anthropic å®¢æˆ·ç«¯ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if Anthropic is None:
            return None
        
        if self._anthropic_client is None:
            if not self.anthropic_api_key:
                return None
            
            self._anthropic_client = Anthropic(api_key=self.anthropic_api_key)
            logger.debug("åˆå§‹åŒ– Anthropic å®¢æˆ·ç«¯")
        
        return self._anthropic_client
    
    def _detect_provider(self, model_name: str) -> str:
        """
        æ£€æµ‹æ¨¡å‹æ‰€å±çš„æä¾›å•†
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ "gpt-4o", "claude-3.5-sonnet", "llama3.2"ï¼‰
            
        Returns:
            æä¾›å•†åç§°ï¼šopenai, anthropic, ollama
            
        Raises:
            LLMError: å¦‚æœæ— æ³•è¯†åˆ«æä¾›å•†
        """
        model_lower = model_name.lower()
        
        # Anthropic æ¨¡å‹
        if "claude" in model_lower:
            return "anthropic"
        
        # Ollama æ¨¡å‹ï¼ˆé€šå¸¸ä¸åŒ…å« "gpt" æˆ– "claude"ï¼‰
        if model_lower.startswith(("llama", "qwen", "mistral", "phi", "gemma")):
            return "ollama"
        
        # OpenAI æ¨¡å‹æˆ–ç¬¬ä¸‰æ–¹å¹³å°ï¼ˆé»˜è®¤ï¼‰
        if any(keyword in model_lower for keyword in ["gpt", "o1", "text-"]):
            return "openai"
        
        # å¦‚æœé…ç½®äº†è‡ªå®šä¹‰ base_urlï¼Œå¯èƒ½æ˜¯ç¬¬ä¸‰æ–¹å¹³å°
        if self.openai_base_url and "openai.com" not in self.openai_base_url.lower():
            return "openai"  # ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
        
        # é»˜è®¤å°è¯• OpenAIï¼ˆåŒ…æ‹¬ç¬¬ä¸‰æ–¹å¹³å°ï¼‰
        return "openai"
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((LLMError, Exception))
    )
    def call_llm(
        self,
        prompt: str,
        model_name: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> str:
        """
        ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ "gpt-4o", "claude-3.5-sonnet", "llama3.2"ï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0.0-2.0ï¼‰ï¼Œé»˜è®¤ 0.7
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°ï¼Œé»˜è®¤ 2000
            **kwargs: å…¶ä»–æ¨¡å‹ç‰¹å®šå‚æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
            
        Raises:
            LLMError: è°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º
            
        Example:
            >>> client = LLMClient()
            >>> result = client.call_llm(
            ...     prompt="åˆ†æè¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘",
            ...     model_name="gpt-4o",
            ...     system_prompt="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æä¸“å®¶"
            ... )
        """
        # Mock æ¨¡å¼æ£€æŸ¥
        from config import DevConfig
        if DevConfig.MOCK_MODE:
            logger.info(f"ğŸ­ Mock æ¨¡å¼ï¼šæ¨¡æ‹Ÿ LLM è°ƒç”¨ ({model_name})")
            from utils.mock_data import get_mock_llm_response
            
            # æ ¹æ®æç¤ºè¯æ¨æ–­ä»»åŠ¡ç±»å‹
            task_type = 'general'
            if 'analyze' in prompt.lower() or 'åˆ†æ' in prompt:
                task_type = 'analysis'
            elif 'create' in prompt.lower() or 'åˆ›ä½œ' in prompt or 'ç”Ÿæˆ' in prompt:
                task_type = 'creation'
            
            return get_mock_llm_response(prompt, task_type)
        
        try:
            provider = self._detect_provider(model_name)
            logger.info(f"è°ƒç”¨ {provider} æ¨¡å‹: {model_name}")
            
            if provider == "openai":
                return self._call_openai(
                    prompt=prompt,
                    model_name=model_name,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    **kwargs
                )
            elif provider == "anthropic":
                return self._call_anthropic(
                    prompt=prompt,
                    model_name=model_name,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    **kwargs
                )
            elif provider == "ollama":
                return self._call_ollama(
                    prompt=prompt,
                    model_name=model_name,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    **kwargs
                )
            else:
                raise LLMError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}")
                
        except Exception as e:
            if isinstance(e, LLMError):
                raise
            error_msg = f"è°ƒç”¨ LLM å¤±è´¥ ({model_name}): {str(e)}"
            logger.error(error_msg, exc_info=True)
            raise LLMError(error_msg) from e
    
    def _call_openai(
        self,
        prompt: str,
        model_name: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int,
        **kwargs
    ) -> str:
        """è°ƒç”¨ OpenAI API"""
        client = self._get_openai_client()
        if client is None:
            raise LLMError("OpenAI å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ OPENAI_API_KEY é…ç½®")
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            content = response.choices[0].message.content
            if not content:
                raise LLMError("OpenAI è¿”å›ç©ºå†…å®¹")
            
            logger.debug(f"OpenAI è°ƒç”¨æˆåŠŸï¼Œç”Ÿæˆ {len(content)} å­—ç¬¦")
            return content
            
        except Exception as e:
            raise LLMError(f"OpenAI API è°ƒç”¨å¤±è´¥: {str(e)}")
    
    def _call_anthropic(
        self,
        prompt: str,
        model_name: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int,
        **kwargs
    ) -> str:
        """è°ƒç”¨ Anthropic API"""
        client = self._get_anthropic_client()
        if client is None:
            raise LLMError("Anthropic å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ ANTHROPIC_API_KEY é…ç½®")
        
        try:
            # Anthropic API çš„æ¶ˆæ¯æ ¼å¼
            messages = [{"role": "user", "content": prompt}]
            
            api_kwargs = {
                "model": model_name,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
            }
            
            # Anthropic çš„ç³»ç»Ÿæç¤ºè¯é€šè¿‡ system å‚æ•°ä¼ é€’
            if system_prompt:
                api_kwargs["system"] = system_prompt
            
            # æ·»åŠ å…¶ä»–å‚æ•°
            api_kwargs.update(kwargs)
            
            response = client.messages.create(**api_kwargs)
            
            if not response.content or len(response.content) == 0:
                raise LLMError("Anthropic è¿”å›ç©ºå†…å®¹")
            
            # Anthropic è¿”å›çš„å†…å®¹æ˜¯åˆ—è¡¨æ ¼å¼
            content = ""
            for block in response.content:
                if hasattr(block, 'text'):
                    content += block.text
                elif isinstance(block, str):
                    content += block
            
            if not content:
                raise LLMError("Anthropic è¿”å›å†…å®¹ä¸ºç©º")
            
            logger.debug(f"Anthropic è°ƒç”¨æˆåŠŸï¼Œç”Ÿæˆ {len(content)} å­—ç¬¦")
            return content
            
        except Exception as e:
            raise LLMError(f"Anthropic API è°ƒç”¨å¤±è´¥: {str(e)}")
    
    def _call_ollama(
        self,
        prompt: str,
        model_name: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int,
        **kwargs
    ) -> str:
        """è°ƒç”¨ Ollama APIï¼ˆé€šè¿‡ OpenAI å…¼å®¹æ¥å£ï¼‰"""
        # Ollama ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
        if not self.ollama_base_url:
            raise LLMError("OLLAMA_BASE_URL æœªé…ç½®")
        
        # ä¸´æ—¶åˆ›å»º OpenAI å®¢æˆ·ç«¯æŒ‡å‘ Ollama
        try:
            if OpenAI is None:
                raise LLMError("openai åº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ Ollama")
            
            ollama_client = OpenAI(
                api_key="ollama",  # Ollama ä¸éœ€è¦çœŸå®çš„ API Key
                base_url=self.ollama_base_url
            )
            
            # æ„å»ºæ¶ˆæ¯
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            response = ollama_client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            content = response.choices[0].message.content
            if not content:
                raise LLMError("Ollama è¿”å›ç©ºå†…å®¹")
            
            logger.debug(f"Ollama è°ƒç”¨æˆåŠŸï¼Œç”Ÿæˆ {len(content)} å­—ç¬¦")
            return content
            
        except Exception as e:
            raise LLMError(f"Ollama API è°ƒç”¨å¤±è´¥: {str(e)}")


# ä¾¿æ·å‡½æ•°ï¼šå¿«é€Ÿåˆ›å»ºå®¢æˆ·ç«¯å¹¶è°ƒç”¨
def call_llm(
    prompt: str,
    model_name: str,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 2000,
    **kwargs
) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šå¿«é€Ÿè°ƒç”¨ LLM
    
    Args:
        prompt: ç”¨æˆ·æç¤ºè¯
        model_name: æ¨¡å‹åç§°
        system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ 0.7
        max_tokens: æœ€å¤§ token æ•°ï¼Œé»˜è®¤ 2000
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        
    Example:
        >>> result = call_llm("å†™ä¸€é¦–è¯—", "gpt-4o-mini")
        >>> print(result)
    """
    client = LLMClient()
    return client.call_llm(
        prompt=prompt,
        model_name=model_name,
        system_prompt=system_prompt,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )


# æ¨¡å—çº§åˆ«çš„å•ä¾‹å®ä¾‹ï¼ˆå¯é€‰ï¼‰
_client_instance = None


def get_client() -> LLMClient:
    """
    è·å–å…¨å±€å•ä¾‹ LLM å®¢æˆ·ç«¯å®ä¾‹
    
    å¦‚æœå®¢æˆ·ç«¯å°šæœªåˆ›å»ºï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°å®ä¾‹
    åç»­è°ƒç”¨å°†è¿”å›åŒä¸€ä¸ªå®ä¾‹
    """
    global _client_instance
    if _client_instance is None:
        _client_instance = LLMClient()
    return _client_instance


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª LLM Client æµ‹è¯•\n")
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯• 1: æ£€æµ‹æä¾›å•†
    print("=" * 60)
    print("æµ‹è¯• 1: æä¾›å•†æ£€æµ‹")
    print("=" * 60)
    
    client = LLMClient()
    test_models = [
        "gpt-4o",
        "claude-3.5-sonnet",
        "llama3.2",
        "gpt-4o-mini"
    ]
    
    for model in test_models:
        provider = client._detect_provider(model)
        print(f"æ¨¡å‹: {model:25} â†’ æä¾›å•†: {provider}")
    
    # æµ‹è¯• 2: å®é™…è°ƒç”¨ï¼ˆéœ€è¦é…ç½® API Keyï¼‰
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: LLM è°ƒç”¨ï¼ˆéœ€è¦é…ç½® API Keyï¼‰")
    print("=" * 60)
    
    test_prompt = "ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½"
    
    # æµ‹è¯• OpenAIï¼ˆå¦‚æœæœ‰é…ç½®ï¼‰
    if ModelConfig.OPENAI_API_KEY:
        try:
            print(f"\næµ‹è¯• OpenAI (gpt-4o-mini)...")
            result = call_llm(
                prompt=test_prompt,
                model_name="gpt-4o-mini",
                max_tokens=100
            )
            print(f"ç»“æœ: {result[:100]}...")
        except Exception as e:
            print(f"âŒ OpenAI è°ƒç”¨å¤±è´¥: {e}")
    else:
        print("âš ï¸  OPENAI_API_KEY æœªé…ç½®ï¼Œè·³è¿‡ OpenAI æµ‹è¯•")
    
    # æµ‹è¯• Anthropicï¼ˆå¦‚æœæœ‰é…ç½®ï¼‰
    if ModelConfig.ANTHROPIC_API_KEY:
        try:
            print(f"\næµ‹è¯• Anthropic (claude-3.5-sonnet)...")
            result = call_llm(
                prompt=test_prompt,
                model_name="claude-3.5-sonnet",
                max_tokens=100
            )
            print(f"ç»“æœ: {result[:100]}...")
        except Exception as e:
            print(f"âŒ Anthropic è°ƒç”¨å¤±è´¥: {e}")
    else:
        print("âš ï¸  ANTHROPIC_API_KEY æœªé…ç½®ï¼Œè·³è¿‡ Anthropic æµ‹è¯•")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")

