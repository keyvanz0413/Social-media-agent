"""
Model Router - æ™ºèƒ½æ¨¡å‹è·¯ç”±å™¨
æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„ LLM æ¨¡å‹

Features:
- è‡ªåŠ¨æ¨¡å‹é€‰æ‹©ï¼ˆåŸºäºä»»åŠ¡ç±»å‹å’Œè´¨é‡è¦æ±‚ï¼‰
- è‡ªåŠ¨é™çº§ç­–ç•¥ï¼ˆä¸»æ¨¡å‹å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢å¤‡ç”¨æ¨¡å‹ï¼‰
- æ¨¡å‹å¥åº·æ£€æŸ¥å’Œå¯ç”¨æ€§æ£€æµ‹
- é‡è¯•æœºåˆ¶å’Œé”™è¯¯æ¢å¤
"""

from enum import Enum
from typing import Dict, Optional, Any, Callable, Tuple
import os
import logging
import time
from functools import wraps
from config import ModelConfig

logger = logging.getLogger(__name__)


class TaskType(Enum):
    """
    ä»»åŠ¡ç±»å‹æšä¸¾
    å®šä¹‰äº†ç³»ç»Ÿæ”¯æŒçš„æ‰€æœ‰ä»»åŠ¡ç±»å‹
    """
    ANALYSIS = "analysis"         # å†…å®¹åˆ†æï¼ˆéœ€è¦æ¨ç†èƒ½åŠ›ï¼‰
    CREATION = "creation"         # å†…å®¹åˆ›ä½œï¼ˆéœ€è¦åˆ›æ„èƒ½åŠ›ï¼‰
    REVIEW = "review"             # å†…å®¹è¯„å®¡ï¼ˆå¿«é€Ÿåˆ¤æ–­ï¼‰
    REASONING = "reasoning"       # å¤æ‚æ¨ç†ï¼ˆç­–ç•¥åˆ¶å®šï¼‰
    VISION = "vision"             # è§†è§‰ç†è§£ï¼ˆå¤šæ¨¡æ€ï¼‰


class QualityLevel(Enum):
    """
    è´¨é‡çº§åˆ«æšä¸¾
    ç”¨äºåœ¨æ€§èƒ½å’Œæˆæœ¬ä¹‹é—´è¿›è¡Œæƒè¡¡
    """
    FAST = "fast"           # å¿«é€Ÿæ¨¡å¼ï¼šä¼˜å…ˆè€ƒè™‘æˆæœ¬ï¼Œä½¿ç”¨è½»é‡æ¨¡å‹
    BALANCED = "balanced"   # å¹³è¡¡æ¨¡å¼ï¼šæ€§èƒ½å’Œæˆæœ¬çš„æœ€ä½³å¹³è¡¡ï¼ˆé»˜è®¤ï¼‰
    HIGH = "high"           # é«˜è´¨é‡æ¨¡å¼ï¼šä¼˜å…ˆè€ƒè™‘æ€§èƒ½ï¼Œä½¿ç”¨æœ€å¼ºæ¨¡å‹


class ModelRouter:
    """
    æ¨¡å‹è·¯ç”±å™¨
    
    èŒè´£ï¼š
    1. æ ¹æ®ä»»åŠ¡ç±»å‹å’Œè´¨é‡è¦æ±‚é€‰æ‹©æœ€ä¼˜æ¨¡å‹
    2. æä¾›æ¨¡å‹é™çº§ç­–ç•¥
    3. æä¾›æ¨¡å‹ä¿¡æ¯æŸ¥è¯¢
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        router = ModelRouter()
        model = router.select_model(TaskType.ANALYSIS)
        result = llm_do(prompt, model=model)
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹è·¯ç”±å™¨ï¼ŒåŠ è½½é…ç½®"""
        self.task_mapping = ModelConfig.TASK_MODEL_MAPPING
        self.fallback_models = ModelConfig.FALLBACK_MODELS
        self.model_info = ModelConfig.MODEL_INFO
        
    def select_model(
        self, 
        task_type: TaskType,
        quality_level: QualityLevel = QualityLevel.BALANCED
    ) -> str:
        """
        æ ¹æ®ä»»åŠ¡ç±»å‹å’Œè´¨é‡è¦æ±‚é€‰æ‹©æ¨¡å‹
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆANALYSIS, CREATION, REVIEWç­‰ï¼‰
            quality_level: è´¨é‡çº§åˆ«ï¼ˆFAST, BALANCED, HIGHï¼‰ï¼Œé»˜è®¤ BALANCED
            
        Returns:
            str: æ¨¡å‹åç§°ï¼Œå¦‚ "gpt-4o", "claude-3.5-sonnet"
            
        Raises:
            ValueError: å¦‚æœä»»åŠ¡ç±»å‹ä¸æ”¯æŒ
            
        Example:
            >>> router = ModelRouter()
            >>> model = router.select_model(TaskType.ANALYSIS)
            >>> print(model)
            'gpt-4o'
            
            >>> model = router.select_model(TaskType.CREATION, QualityLevel.FAST)
            >>> print(model)
            'gpt-4o-mini'
        """
        task_key = task_type.value
        quality_key = quality_level.value
        
        # æ£€æŸ¥ä»»åŠ¡ç±»å‹æ˜¯å¦æ”¯æŒ
        if task_key not in self.task_mapping:
            raise ValueError(
                f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_key}ã€‚"
                f"æ”¯æŒçš„ç±»å‹: {list(self.task_mapping.keys())}"
            )
        
        # è·å–è¯¥ä»»åŠ¡ç±»å‹çš„æ¨¡å‹é…ç½®
        task_models = self.task_mapping[task_key]
        
        # æ£€æŸ¥è´¨é‡çº§åˆ«æ˜¯å¦å­˜åœ¨
        if quality_key not in task_models:
            # å¦‚æœæŒ‡å®šçš„è´¨é‡çº§åˆ«ä¸å­˜åœ¨ï¼Œé™çº§åˆ° balanced
            quality_key = "balanced"
        
        model = task_models[quality_key]
        
        return model
    
    def get_fallback_model(self, primary_model: str) -> Optional[str]:
        """
        è·å–å¤‡ç”¨æ¨¡å‹ï¼ˆé™çº§ç­–ç•¥ï¼‰
        
        å½“ä¸»æ¨¡å‹è°ƒç”¨å¤±è´¥æ—¶ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
        
        Args:
            primary_model: ä¸»æ¨¡å‹åç§°
            
        Returns:
            Optional[str]: å¤‡ç”¨æ¨¡å‹åç§°ï¼Œå¦‚æœæ²¡æœ‰å¤‡ç”¨æ¨¡å‹åˆ™è¿”å› None
            
        Example:
            >>> router = ModelRouter()
            >>> fallback = router.get_fallback_model("gpt-4o")
            >>> print(fallback)
            'gpt-4o-mini'
            
            >>> fallback = router.get_fallback_model("gpt-4o-mini")
            >>> print(fallback)
            None  # å·²ç»æ˜¯æœ€ä¾¿å®œçš„æ¨¡å‹ï¼Œæ— æ³•ç»§ç»­é™çº§
        """
        return self.fallback_models.get(primary_model)
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            Dict: åŒ…å«æ¨¡å‹è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
                - provider: æä¾›å•†ï¼ˆopenai, anthropicç­‰ï¼‰
                - description: æ¨¡å‹æè¿°
                - strengths: ä¼˜åŠ¿åˆ—è¡¨
                - cost_level: æˆæœ¬çº§åˆ«ï¼ˆlow, medium, highï¼‰
                - context_window: ä¸Šä¸‹æ–‡çª—å£å¤§å°
                
        Example:
            >>> router = ModelRouter()
            >>> info = router.get_model_info("gpt-4o")
            >>> print(info['description'])
            'OpenAI æœ€æ–°æ——èˆ°æ¨¡å‹'
            >>> print(info['strengths'])
            ['æ·±åº¦æ¨ç†', 'å¤æ‚é—®é¢˜æ±‚è§£', 'ç­–ç•¥åˆ¶å®š']
        """
        if model_name not in self.model_info:
            return {
                "provider": "unknown",
                "description": f"æœªçŸ¥æ¨¡å‹: {model_name}",
                "strengths": [],
                "cost_level": "unknown",
                "context_window": 0
            }
        
        return self.model_info[model_name]
    
    def get_all_models(self) -> list[str]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        
        Returns:
            list: æ‰€æœ‰æ¨¡å‹åç§°çš„åˆ—è¡¨
        """
        return list(self.model_info.keys())
    
    def get_models_by_task(self, task_type: TaskType) -> Dict[str, str]:
        """
        è·å–æŒ‡å®šä»»åŠ¡ç±»å‹çš„æ‰€æœ‰è´¨é‡çº§åˆ«å¯¹åº”çš„æ¨¡å‹
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            
        Returns:
            Dict: è´¨é‡çº§åˆ«åˆ°æ¨¡å‹åç§°çš„æ˜ å°„
            
        Example:
            >>> router = ModelRouter()
            >>> models = router.get_models_by_task(TaskType.CREATION)
            >>> print(models)
            {'fast': 'gpt-4o-mini', 'balanced': 'claude-3.5-sonnet', 'high': 'claude-3.5-sonnet'}
        """
        task_key = task_type.value
        return self.task_mapping.get(task_key, {})
    
    def suggest_model(
        self, 
        task_description: str,
        prefer_fast: bool = False
    ) -> str:
        """
        æ ¹æ®ä»»åŠ¡æè¿°æ™ºèƒ½æ¨èæ¨¡å‹ï¼ˆç®€å•çš„å¯å‘å¼è§„åˆ™ï¼‰
        
        Args:
            task_description: ä»»åŠ¡æè¿°æ–‡æœ¬
            prefer_fast: æ˜¯å¦ä¼˜å…ˆè€ƒè™‘é€Ÿåº¦
            
        Returns:
            str: æ¨èçš„æ¨¡å‹åç§°
            
        Note:
            è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œæœªæ¥å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„å¯å‘å¼æˆ–æœºå™¨å­¦ä¹ æ–¹æ³•
        """
        desc_lower = task_description.lower()
        
        # å…³é”®è¯åŒ¹é…ï¼ˆç®€å•çš„å¯å‘å¼è§„åˆ™ï¼‰
        if any(word in desc_lower for word in ['åˆ†æ', 'analyze', 'ç ”ç©¶', 'study']):
            task_type = TaskType.ANALYSIS
        elif any(word in desc_lower for word in ['åˆ›ä½œ', 'create', 'å†™ä½œ', 'write', 'ç”Ÿæˆ', 'generate']):
            task_type = TaskType.CREATION
        elif any(word in desc_lower for word in ['è¯„å®¡', 'review', 'æ£€æŸ¥', 'check', 'è¯„åˆ†', 'score']):
            task_type = TaskType.REVIEW
        elif any(word in desc_lower for word in ['å›¾ç‰‡', 'image', 'è§†è§‰', 'visual', 'å›¾åƒ']):
            task_type = TaskType.VISION
        else:
            # é»˜è®¤ä½¿ç”¨æ¨ç†ä»»åŠ¡
            task_type = TaskType.REASONING
        
        # æ ¹æ®é€Ÿåº¦åå¥½é€‰æ‹©è´¨é‡çº§åˆ«
        quality = QualityLevel.FAST if prefer_fast else QualityLevel.BALANCED
        
        return self.select_model(task_type, quality)
    
    def get_fallback_chain(self, primary_model: str, max_depth: int = 5) -> list[str]:
        """
        è·å–å®Œæ•´çš„é™çº§é“¾
        
        Args:
            primary_model: ä¸»æ¨¡å‹åç§°
            max_depth: æœ€å¤§é™çº§æ·±åº¦ï¼Œé˜²æ­¢å¾ªç¯å¼•ç”¨
            
        Returns:
            list: é™çº§é“¾åˆ—è¡¨ï¼Œä»ä¸»æ¨¡å‹åˆ°æœ€ç»ˆå¤‡ç”¨æ¨¡å‹
            
        Example:
            >>> router = ModelRouter()
            >>> chain = router.get_fallback_chain("gpt-4o")
            >>> print(chain)
            ['gpt-4o', 'gpt-4o-mini']
        """
        chain = [primary_model]
        current = primary_model
        depth = 0
        
        while depth < max_depth:
            fallback = self.get_fallback_model(current)
            if fallback is None or fallback in chain:
                break
            chain.append(fallback)
            current = fallback
            depth += 1
        
        return chain
    
    def call_with_fallback(
        self,
        model_name: str,
        call_function: Callable,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        **kwargs
    ) -> Tuple[Any, str]:
        """
        ä½¿ç”¨è‡ªåŠ¨é™çº§ç­–ç•¥è°ƒç”¨ LLM
        
        å½“ä¸»æ¨¡å‹è°ƒç”¨å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨å°è¯•é™çº§é“¾ä¸­çš„å¤‡ç”¨æ¨¡å‹ï¼Œ
        å¹¶æ”¯æŒæ¯ä¸ªæ¨¡å‹çš„é‡è¯•æœºåˆ¶ã€‚
        
        Args:
            model_name: ä¸»æ¨¡å‹åç§°
            call_function: è°ƒç”¨å‡½æ•°ï¼Œç­¾ååº”ä¸º func(model=..., **kwargs)
            max_retries: æ¯ä¸ªæ¨¡å‹çš„æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            **kwargs: ä¼ é€’ç»™ call_function çš„é¢å¤–å‚æ•°
            
        Returns:
            Tuple[Any, str]: (è°ƒç”¨ç»“æœ, æˆåŠŸä½¿ç”¨çš„æ¨¡å‹åç§°)
            
        Raises:
            Exception: å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªå¼‚å¸¸
            
        Example:
            >>> def my_llm_call(model, prompt):
            ...     # ä½ çš„ LLM è°ƒç”¨é€»è¾‘
            ...     return llm.chat(model=model, messages=[{"role": "user", "content": prompt}])
            >>> 
            >>> router = ModelRouter()
            >>> result, used_model = router.call_with_fallback(
            ...     "gpt-4o",
            ...     my_llm_call,
            ...     prompt="åˆ†æè¿™æ®µæ–‡æœ¬"
            ... )
            >>> print(f"ä½¿ç”¨æ¨¡å‹: {used_model}")
            >>> print(f"ç»“æœ: {result}")
        """
        fallback_chain = self.get_fallback_chain(model_name)
        last_exception = None
        
        for model in fallback_chain:
            logger.info(f"å°è¯•ä½¿ç”¨æ¨¡å‹: {model}")
            
            # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œé‡è¯•
            for attempt in range(max_retries):
                try:
                    result = call_function(model=model, **kwargs)
                    
                    # æˆåŠŸï¼è®°å½•å¹¶è¿”å›
                    if model != model_name:
                        logger.warning(
                            f"ä¸»æ¨¡å‹ {model_name} ä¸å¯ç”¨ï¼Œå·²é™çº§è‡³ {model}"
                        )
                    else:
                        logger.info(f"æˆåŠŸä½¿ç”¨æ¨¡å‹: {model}")
                    
                    return result, model
                    
                except Exception as e:
                    last_exception = e
                    logger.warning(
                        f"æ¨¡å‹ {model} è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}"
                    )
                    
                    # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡é‡è¯•ï¼Œç­‰å¾…åå†è¯•
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
            
            # è¿™ä¸ªæ¨¡å‹çš„æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¤‡ç”¨æ¨¡å‹
            logger.error(f"æ¨¡å‹ {model} åœ¨ {max_retries} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥ï¼Œå°è¯•é™çº§")
        
        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†
        logger.error(f"æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†ï¼é™çº§é“¾: {fallback_chain}")
        raise Exception(
            f"æ‰€æœ‰æ¨¡å‹è°ƒç”¨å¤±è´¥ã€‚æœ€åé”™è¯¯: {str(last_exception)}"
        )
    
    def check_model_availability(
        self,
        model_name: str,
        test_function: Optional[Callable] = None,
        timeout: float = 10.0
    ) -> bool:
        """
        æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
        
        Args:
            model_name: è¦æ£€æŸ¥çš„æ¨¡å‹åç§°
            test_function: æµ‹è¯•å‡½æ•°ï¼Œç”¨äºå®é™…è°ƒç”¨æ¨¡å‹
                         å¦‚æœä¸º Noneï¼Œåªæ£€æŸ¥é…ç½®æ˜¯å¦å­˜åœ¨
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            bool: æ¨¡å‹æ˜¯å¦å¯ç”¨
            
        Example:
            >>> def test_call(model):
            ...     return llm.chat(model=model, messages=[{"role": "user", "content": "test"}])
            >>> 
            >>> router = ModelRouter()
            >>> is_available = router.check_model_availability("gpt-4o", test_call)
            >>> print(f"GPT-4o å¯ç”¨: {is_available}")
        """
        # 1. æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨é…ç½®ä¸­
        if model_name not in self.model_info:
            logger.warning(f"æ¨¡å‹ {model_name} ä¸åœ¨é…ç½®ä¸­")
            return False
        
        # 2. æ£€æŸ¥ API key æ˜¯å¦é…ç½®
        model_info = self.get_model_info(model_name)
        provider = model_info.get('provider', 'unknown')
        
        if provider == 'openai' and not ModelConfig.OPENAI_API_KEY:
            logger.warning(f"æ¨¡å‹ {model_name} éœ€è¦ OPENAI_API_KEY")
            return False
        elif provider == 'anthropic' and not ModelConfig.ANTHROPIC_API_KEY:
            logger.warning(f"æ¨¡å‹ {model_name} éœ€è¦ ANTHROPIC_API_KEY")
            return False
        
        # 3. å¦‚æœæä¾›äº†æµ‹è¯•å‡½æ•°ï¼Œå®é™…è°ƒç”¨æµ‹è¯•
        if test_function:
            try:
                import signal
                
                # è®¾ç½®è¶…æ—¶
                def timeout_handler(signum, frame):
                    raise TimeoutError("æ¨¡å‹æµ‹è¯•è¶…æ—¶")
                
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(int(timeout))
                
                try:
                    test_function(model=model_name)
                    signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                    logger.info(f"æ¨¡å‹ {model_name} å¯ç”¨")
                    return True
                except Exception as e:
                    signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                    logger.warning(f"æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: {str(e)}")
                    return False
                    
            except Exception as e:
                logger.warning(f"æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥å¤±è´¥: {str(e)}")
                return False
        
        # å¦‚æœæ²¡æœ‰æµ‹è¯•å‡½æ•°ï¼Œåªæ£€æŸ¥é…ç½®
        return True
    
    def get_available_models(
        self,
        test_function: Optional[Callable] = None
    ) -> Dict[str, bool]:
        """
        è·å–æ‰€æœ‰æ¨¡å‹çš„å¯ç”¨æ€§çŠ¶æ€
        
        Args:
            test_function: å¯é€‰çš„æµ‹è¯•å‡½æ•°
            
        Returns:
            Dict: æ¨¡å‹åç§°åˆ°å¯ç”¨æ€§çš„æ˜ å°„
            
        Example:
            >>> router = ModelRouter()
            >>> availability = router.get_available_models()
            >>> for model, is_available in availability.items():
            ...     status = "âœ…" if is_available else "âŒ"
            ...     print(f"{status} {model}")
        """
        availability = {}
        
        for model_name in self.get_all_models():
            availability[model_name] = self.check_model_availability(
                model_name,
                test_function
            )
        
        return availability
    
    def print_info(self):
        """
        æ‰“å°è·¯ç”±å™¨é…ç½®ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        """
        print("=" * 60)
        print("Model Router é…ç½®ä¿¡æ¯")
        print("=" * 60)
        
        print("\nğŸ“‹ æ”¯æŒçš„ä»»åŠ¡ç±»å‹:")
        for task in TaskType:
            print(f"  - {task.value}")
        
        print("\nğŸ“Š è´¨é‡çº§åˆ«:")
        for level in QualityLevel:
            print(f"  - {level.value}")
        
        print("\nğŸ¤– å¯ç”¨æ¨¡å‹:")
        for model_name in self.get_all_models():
            info = self.get_model_info(model_name)
            print(f"  - {model_name}")
            print(f"    æä¾›å•†: {info['provider']}")
            print(f"    æè¿°: {info['description']}")
            print(f"    æˆæœ¬: {info['cost_level']}")
        
        print("\nğŸ”„ é™çº§é“¾:")
        for primary, fallback in self.fallback_models.items():
            if fallback:
                fallback_chain = self.get_fallback_chain(primary)
                chain_str = " â†’ ".join(fallback_chain)
                print(f"  {chain_str}")
            else:
                print(f"  {primary} â†’ (æ— å¤‡ç”¨)")
        
        print("=" * 60)


# ä¾¿æ·å‡½æ•°ï¼šå¿«é€Ÿåˆ›å»ºè·¯ç”±å™¨å®ä¾‹
def create_router() -> ModelRouter:
    """
    åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ª ModelRouter å®ä¾‹
    
    è¿™æ˜¯ä¸€ä¸ªä¾¿æ·å‡½æ•°ï¼Œç”¨äºå¿«é€Ÿè·å–è·¯ç”±å™¨å®ä¾‹
    """
    return ModelRouter()


# æ¨¡å—çº§åˆ«çš„å•ä¾‹å®ä¾‹ï¼ˆå¯é€‰ï¼‰
_router_instance = None

def get_router() -> ModelRouter:
    """
    è·å–å…¨å±€å•ä¾‹è·¯ç”±å™¨å®ä¾‹
    
    å¦‚æœè·¯ç”±å™¨å°šæœªåˆ›å»ºï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°å®ä¾‹
    åç»­è°ƒç”¨å°†è¿”å›åŒä¸€ä¸ªå®ä¾‹
    """
    global _router_instance
    if _router_instance is None:
        _router_instance = ModelRouter()
    return _router_instance


# è£…é¥°å™¨ï¼šè‡ªåŠ¨é™çº§
def with_fallback(
    model_name: str,
    max_retries: int = 3,
    retry_delay: float = 1.0
):
    """
    è£…é¥°å™¨ï¼šä¸ºå‡½æ•°æ·»åŠ è‡ªåŠ¨é™çº§åŠŸèƒ½
    
    Args:
        model_name: ä¸»æ¨¡å‹åç§°
        max_retries: æ¯ä¸ªæ¨¡å‹çš„æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        
    Example:
        >>> @with_fallback("gpt-4o", max_retries=3)
        ... def analyze_content(model: str, prompt: str):
        ...     return llm.chat(model=model, messages=[{"role": "user", "content": prompt}])
        >>> 
        >>> result = analyze_content(prompt="åˆ†æè¿™æ®µæ–‡æœ¬")
        >>> # å¦‚æœ gpt-4o å¤±è´¥ï¼Œä¼šè‡ªåŠ¨å°è¯• gpt-4o-mini
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            router = get_router()
            
            # åˆ›å»ºè°ƒç”¨å‡½æ•°
            def call_func(model: str, **kw):
                return func(*args, model=model, **{**kwargs, **kw})
            
            result, used_model = router.call_with_fallback(
                model_name,
                call_func,
                max_retries=max_retries,
                retry_delay=retry_delay
            )
            
            return result
        
        return wrapper
    return decorator


def select_best_available_model(
    task_type: TaskType,
    quality_level: QualityLevel = QualityLevel.BALANCED,
    test_function: Optional[Callable] = None
) -> str:
    """
    é€‰æ‹©æœ€ä½³å¯ç”¨æ¨¡å‹
    
    å¦‚æœé¦–é€‰æ¨¡å‹ä¸å¯ç”¨ï¼Œè‡ªåŠ¨é€‰æ‹©é™çº§é“¾ä¸­çš„ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
    
    Args:
        task_type: ä»»åŠ¡ç±»å‹
        quality_level: è´¨é‡çº§åˆ«
        test_function: å¯é€‰çš„æµ‹è¯•å‡½æ•°
        
    Returns:
        str: ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹åç§°
        
    Example:
        >>> model = select_best_available_model(TaskType.ANALYSIS, QualityLevel.HIGH)
        >>> print(f"å°†ä½¿ç”¨æ¨¡å‹: {model}")
    """
    router = get_router()
    
    # é€‰æ‹©é¦–é€‰æ¨¡å‹
    preferred_model = router.select_model(task_type, quality_level)
    
    # è·å–é™çº§é“¾
    fallback_chain = router.get_fallback_chain(preferred_model)
    
    # æ£€æŸ¥æ¯ä¸ªæ¨¡å‹çš„å¯ç”¨æ€§
    for model in fallback_chain:
        if router.check_model_availability(model, test_function):
            if model != preferred_model:
                logger.info(
                    f"é¦–é€‰æ¨¡å‹ {preferred_model} ä¸å¯ç”¨ï¼Œä½¿ç”¨ {model}"
                )
            return model
    
    # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨ï¼Œè¿”å›é¦–é€‰æ¨¡å‹ï¼ˆè®©åç»­è°ƒç”¨å¤„ç†é”™è¯¯ï¼‰
    logger.warning(
        f"é™çº§é“¾ä¸­æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨: {fallback_chain}ï¼Œè¿”å›é¦–é€‰æ¨¡å‹"
    )
    return preferred_model


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª Model Router æµ‹è¯•\n")
    
    router = ModelRouter()
    
    # æµ‹è¯• 1: åŸºç¡€æ¨¡å‹é€‰æ‹©
    print("=" * 60)
    print("æµ‹è¯• 1: åŸºç¡€æ¨¡å‹é€‰æ‹©")
    print("=" * 60)
    
    test_cases = [
        (TaskType.ANALYSIS, QualityLevel.BALANCED),
        (TaskType.CREATION, QualityLevel.HIGH),
        (TaskType.REVIEW, QualityLevel.FAST),
    ]
    
    for task, quality in test_cases:
        model = router.select_model(task, quality)
        print(f"ä»»åŠ¡: {task.value:12} | è´¨é‡: {quality.value:10} â†’ æ¨¡å‹: {model}")
    
    # æµ‹è¯• 2: é™çº§é“¾ï¼ˆå¢å¼ºç‰ˆï¼‰
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: é™çº§é“¾ï¼ˆå®Œæ•´è·¯å¾„ï¼‰")
    print("=" * 60)
    
    models_to_test = ["gpt-4o", "claude-3-5-sonnet-20241022", "gpt-4o-mini"]
    for model in models_to_test:
        chain = router.get_fallback_chain(model)
        chain_str = " â†’ ".join(chain)
        print(f"{model:30} â†’ {chain_str}")
    
    # æµ‹è¯• 3: æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥ï¼ˆä»…é…ç½®æ£€æŸ¥ï¼‰")
    print("=" * 60)
    
    availability = router.get_available_models()
    for model_name, is_available in availability.items():
        status = "âœ…" if is_available else "âŒ"
        print(f"{status} {model_name}")
    
    # æµ‹è¯• 4: æ¨¡å‹ä¿¡æ¯
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: æ¨¡å‹ä¿¡æ¯æŸ¥è¯¢")
    print("=" * 60)
    
    info = router.get_model_info("gpt-4o")
    print(f"æ¨¡å‹: gpt-4o")
    print(f"  æè¿°: {info['description']}")
    print(f"  ä¼˜åŠ¿: {', '.join(info['strengths'])}")
    print(f"  æˆæœ¬: {info['cost_level']}")
    print(f"  ä¸Šä¸‹æ–‡çª—å£: {info['context_window']:,} tokens")
    
    # æµ‹è¯• 5: æ™ºèƒ½æ¨è
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: æ™ºèƒ½æ¨è")
    print("=" * 60)
    
    tasks = [
        "åˆ†æè¿™ç¯‡æ–‡ç« çš„ä¸»è¦è§‚ç‚¹",
        "åˆ›ä½œä¸€ç¯‡å°çº¢ä¹¦å¸–å­",
        "è¯„å®¡è¿™æ®µä»£ç çš„è´¨é‡"
    ]
    
    for task_desc in tasks:
        recommended = router.suggest_model(task_desc)
        print(f"ä»»åŠ¡: {task_desc:30} â†’ æ¨è: {recommended}")
    
    # æµ‹è¯• 6: é€‰æ‹©æœ€ä½³å¯ç”¨æ¨¡å‹
    print("\n" + "=" * 60)
    print("æµ‹è¯• 6: é€‰æ‹©æœ€ä½³å¯ç”¨æ¨¡å‹")
    print("=" * 60)
    
    test_tasks = [
        (TaskType.ANALYSIS, QualityLevel.HIGH),
        (TaskType.CREATION, QualityLevel.BALANCED),
        (TaskType.REVIEW, QualityLevel.FAST),
    ]
    
    for task, quality in test_tasks:
        best_model = select_best_available_model(task, quality)
        preferred = router.select_model(task, quality)
        if best_model != preferred:
            print(f"ä»»åŠ¡: {task.value:12} | é¦–é€‰: {preferred:30} â†’ å®é™…: {best_model}")
        else:
            print(f"ä»»åŠ¡: {task.value:12} | æ¨¡å‹: {best_model}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print("  1. call_with_fallback() - è‡ªåŠ¨é™çº§è°ƒç”¨")
    print("  2. @with_fallback() - è£…é¥°å™¨æ¨¡å¼")
    print("  3. select_best_available_model() - æ™ºèƒ½é€‰æ‹©")
